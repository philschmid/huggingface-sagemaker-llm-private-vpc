{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Large Language Models (LLMs) to Amazon SageMaker using new Hugging Face LLM DLC from Amazon S3\n",
    "\n",
    "This is an example on how to deploy the open-source LLMs to Amazon SageMaker for inference using the new Hugging Face LLM Inference Container from Amazon S3. The new Hugging Face LLM Inference Container makes it super easy to deploy LLMs by simply providing a `HF_MODEL_ID` pointing to the Hugging Face Repository and the container takes care of the rest. \n",
    "But for some workloads you cannot load the model from Hugging face Hub and need to load your model from Amazon S3 since there is not internet access for your endpoint. \n",
    "\n",
    "This examples demonstrate how to deploy an open-source LLM from Amazon S3 to Amazon SageMaker using the new Hugging Face LLM Inference Container. We are going to deploy the [HuggingFaceH4/starchat-beta](https://huggingface.co/HuggingFaceH4/starchat-beta). \n",
    "\n",
    "The example covers:\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Upload the model to Amazon S3](#2-upload-the-model-to-amazon-s3)\n",
    "3. [Retrieve the new Hugging Face LLM DLC](#3-retrieve-the-new-hugging-face-llm-dlc)\n",
    "4. [Deploy Starchat-beta to Amazon SageMaker](#4-deploy-starchat-beta-to-amazon-sagemaker)\n",
    "5. [Test the model and run inference](#5-test-the-model-and-run-inference)\n",
    "6. [Clean up](#6-clean-up)\n",
    "\n",
    "\n",
    "If you want to learn more about the Hugging Face LLM Inference DLC check out the introduction [here](https://huggingface.co/blog/sagemaker-huggingface-llm). Lets get started!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy [HuggingFaceH4/starchat-beta](https://huggingface.co/HuggingFaceH4/starchat-beta). to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.163.0\" \"huggingface_hub\" \"hf-transfer\" --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload the model to Amazon S3\n",
    "\n",
    "To deploy our model from Amazon S3 we need to bundle it together with our model weights into a `model.tar.gz`. The Hugging Face LLM inference DLCs uses safetensors weights to load models. Due to the filesystem constraints of `/opt/ml/model` we need to make sure the model we want to deploy has `*.safetensors` weights available. \n",
    "If the model, e.g. [google/flan-ul2](https://huggingface.co/google/flan-ul2) has no `safetensors` weights available we can use the [safetensors/convert_large](https://huggingface.co/spaces/safetensors/convert_large) space to create them. The Space will open a PR on the original repository with the `safetensors` weights, which means we can use the `safetensors` weights from the original repository via the `revision` parameter.\n",
    "_Note: Depending on the size the conversion can take ~10 minutes._\n",
    "\n",
    "Alternative you can save directly to `safetensors` with `model.save_pretrained(..., safe_serialization=True)` with `safetenors` installed during your training. \n",
    "\n",
    "The `model.tar.gz` archive includes all our model-artifcats to run inference. We will use the `huggingface_hub` SDK to easily download `HuggingFaceH4/starchat-beta` from Hugging Face and then upload it to Amazon S3 with the sagemaker SDK.\n",
    "\n",
    "Make sure the enviornment has enough diskspace to store the model, ~35GB should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# set HF_HUB_ENABLE_HF_TRANSFER env var to enable hf-transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"HuggingFaceH4/starchat-beta\"\n",
    "# create model dir\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(\n",
    "    HF_MODEL_ID,\n",
    "    local_dir=str(model_tar_dir), # download to model dir\n",
    "    revision=\"main\", # use a specific revision, e.g. refs/pr/21\n",
    "    local_dir_use_symlinks=False, # use no symlinks to save disk space\n",
    "    ignore_patterns=[\"*.msgpack*\", \"*.h5*\", \"*.bin*\"], # to load safetensor weights\n",
    ")\n",
    "\n",
    "# check if safetensor weights are downloaded and available\n",
    "assert len(list(model_tar_dir.glob(\"*.safetensors\"))) > 0, \"Model download failed\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important is that the archive should directly contain all files and not a folder with the files. For example, your file should look like this:\n",
    "```\n",
    "model.tar.gz/\n",
    "|- config.json\n",
    "|- model-00001-of-00005.safetensors\n",
    "|- tokenizer.json\n",
    "|- ...\n",
    "```\n",
    "\n",
    "We are using `pigz` to parallelize the archiving. _Note: you might need to install it, e.g. `apt install pigz`._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir=os.getcwd()\n",
    "# change to model dir\n",
    "os.chdir(str(model_tar_dir))\n",
    "# use pigz for faster and parallel compression\n",
    "!tar -cf model.tar.gz --use-compress-program=pigz *\n",
    "# change back to parent dir\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created the `model.tar.gz` archive we can upload it to Amazon S3. We will use the `sagemaker` SDK to upload the model to our sagemaker session bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")), desired_s3_uri=f\"s3://{sess.default_bucket()}/starchat-beta\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve the new Hugging Face LLM DLC\n",
    "\n",
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Starchat-beta to Amazon SageMaker\n",
    "\n",
    "To deploy [HuggingFaceH4/starchat-beta](https://huggingface.co/HuggingFaceH4/starchat-beta) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `g5.12xlarge` instance type, which has 4 NVIDIA A10G GPUs and 96GB of GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\",# Comment in to quantize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data=s3_model_uri,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.12xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the model and run inference\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find a list of parameters in the [announcement blog post](https://huggingface.co/blog/sagemaker-huggingface-llm). or as part of the [swagger documentation](https://huggingface.github.io/text-generation-inference/)\n",
    "\n",
    "The `starchat-beta` is a conversation model for answering coding question we can simply prompt by asking our question:\n",
    "  \n",
    "```\n",
    "<|system|>\\n You are an Python Expert<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\n",
    "```\n",
    "\n",
    "lets give it a first try and ask how to filter a list in python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can filter a list of dictionaries using the filter() function in Python. Here's an example\n"
     ]
    }
   ],
   "source": [
    "query = \"How can i filter a list of dictionaries?\"\n",
    "\n",
    "res = llm.predict({\n",
    "\t\"inputs\": f\"<|system|>\\n You are an Python Expert<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "})\n",
    "\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. This can be used to have the model stop the generation after the turn of the `bot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can filter a list of dictionaries using the filter() function in Python. Here's an example:\n",
      "``` \n",
      "def filter_dict(my_list, key, value):\n",
      "    return dict(item for item in my_list if item[key] == value)\n",
      "\n",
      "my_list = [\n",
      "    {'name': 'Alice', 'age': 25}, \n",
      "    {'name': 'Bob', 'age': 30}, \n",
      "    {'name': 'Charlie', 'age': 35}\n",
      "]\n",
      "\n",
      "result = filter_dict(my_list, 'age', 30)\n",
      "print(result)\n",
      "# Output: {'name': 'Bob', 'age': 30}\n",
      "``` \n",
      "In this example, the filter_dict() function takes a list of dictionaries, a key, and a value as arguments. It then creates a new list consisting of only the dictionary items where the specified key has the specified value.<|end|>\n"
     ]
    }
   ],
   "source": [
    "# define payload\n",
    "prompt=f\"<|system|>\\n You are an Python Expert<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"<|end|>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print(response[0][\"generated_text\"][:-len(\"<human>:\")])\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! ðŸš€ We have successfully deployed our model from Amazon S3 to Amazon SageMaker and run inference on it. Now, its time for you to try it out yourself and build Generation AI applications with the new Hugging Face LLM DLC on Amazon SageMaker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
